---
title: "A Practical Guide to Function-Valued Traits"
author: "Anika Watson"
date: '2018-03-08'
output:
  rmarkdown::pdf_document:
    fig_caption: yes        
    includes:  
      in_header: preamble-latex.tex
---

```{r echo=FALSE,warning=FALSE}
 library(knitr)
  opts_chunk$set(echo = TRUE,
                 external=TRUE,
                 fig.pos='H'
                 )
```

```{r functions, include=FALSE}
# A function for captioning and referencing images
fig <- local({
    i <- 0
    ref <- list()
    list(
        cap=function(refName, text) {
            i <<- i + 1
            ref[[refName]] <<- i
            paste(text)
        },
        ref=function(refName) {
            ref[[refName]]
        })
})
``` 

```{r, include=FALSE}
library("scales")
library(shiny)
```

## Introduction
##Throwing the baby out with the bathwater. 

Despite our best attempts to conduct thorough studies, many biologists are unwittingly throwing away valuable data on a regular basis. One major cause for this loss is that many biological traits are often described by single measurements when they are, in fact, too complex for such a simple framework. Consider, for instance, that you are comparing two populations of spinach to determine if the leaf sizes differ significantly. An obvious method to conduct this experiment would be to measure the length or area of mature leaves and compare the averages of the two populations with a t-test. This, however, ignores the fact that one population might grow to maturity well before the other does, even if the two populations reach roughly the same size in the end Figure 1. In such a case, the t-test would support a rejection of the null hypothesis when, in fact, the populations did differ significantly, not in the overall amount of growth performed by each leaf, but by the underlying growth curves. Traits of this type that can be described as functions have been termed function-valued traits (Pletcher and Geyer 1999. Treating these traits as functions, rather than single data points, has many statistical advantages including increased power and others, which we will discuss shortly (Stinchcombe et al., 2012; Griswoldet al., 2008). Unfortunately, due to its perceived mathematical sophistication biologists have been hesitant to adopt function-valued methods. The good news is that even relatively simple function-valued analyses can provide valuable insight compared to traditional approaches (Griswold et al., 2008; Baker et al., 2015). This guide aims to serve as an introduction to function-valued trait analysis providing thorough examples including R code to dispel fear of this topic.

```{r, echo=FALSE, fig.pos='H', fig.cap=fig$cap("Growth of spinach leaves1", "A plot of the leaf lengths of two populations of spinach.")}
RcticChR <- function(a, k, g, x) {
  0.5+10*(a*exp(-exp(-k*(x-g)))-exp(-exp(-k*(-g))))
}
Time <- seq(from = 0.5, to = 10, by = 0.5)
inp <- seq(from = 0.5, to = 10, by = 0.5)
a1 <- 1
k1 <- 1
g1 <- 5
Length <- data.frame(matrix(NA, nrow=1, ncol=20))
Mono1akg <- data.frame(matrix(NA, nrow=100, ncol=3))
set.seed(1235)
MonoError <- rnorm(n=20, mean=0, sd=0.4) #measurement error
for (l in 1:20){
  Length[1,l] <- RcticChR(a1, k1, g1, (l/2)) + MonoError[l]
}
a2 <- 1
k2 <- 1
g2 <- 1
RcticChR2 <- data.frame(matrix(NA, nrow=1, ncol=20))
Mono2akg <- data.frame(matrix(NA, nrow=100, ncol=3))
set.seed(1234)
MonoError <- rnorm(n=20, mean=0, sd=0.24) #measurement error
for (l in 1:20){
  RcticChR2[1,l] <- RcticChR(a2, k2, g2, (l/2)) + MonoError[l]
}

plot(y = Length, x = Time, ylim = c(0, 15), xlim = c(0, 10))
points(y = RcticChR2, x = inp, pch = 9)
```


This approach first appeared in literature as far back as 1989 when Mark Kirkpatrick and David Lofsvold noticed that many evolutionarily important traits could be described as functions (Kirkpatrick & Lofsvol, 1989). This caught the eyes of breeders, biologists and statisticians gaining support from various fields as scientists realized just how widespread function-valued traits are. “A function-valued trait is any trait that changes in response to another variable” (Stinchcombe et al., 2012), so given that this includes any traits that change with respect to time, gene expression, or environmental conditions there is no shortage of examples. In a 1998 article W.G. Hill predicted that function-valued methods may eventually replace traditional methods in evolutionary biology (Griswold et al., 2008). The term "function-valued traits" was coined the very next year (Pletcher and Geyer, 1999). 

According to a paper by the "Function-valued Traits Working Group," the function-valued perspective receives such praise because it "offers enhanced statistical power, greater ability to detect genetic constraints, and improved understanding of phenotypic and genetic variation in environmentally sensitive traits" compared to traditional multivariate methods (Stinchcombe & Kirkpatrick, 2012). These advantages come from the fact that fitting a function to data takes into account the ordering and spacing of the data where multivariable methods ignore this information (Griswold et al., 2008), and because fitting a curve to data smooths noise, reducing its effects on analysis (Stinchcombe et al., 2012). Another advantage of function-valued analysis is that it does not care which points the researcher selected to sample their data. One can just as easily fit a curve estimating the growth of a plant if its height is measured on odd days, as if its height were to be measured on even days. This allows for comparison between data sets with different numbers of samples, and whose samples were taken at different points.

Despite these clear advantages, and Hill's prediction, function-valued trait analysis still hasn't taken off. A quick online search for "function-valued traits" returns 7 results between 1990 and 2000, 236 results between 2000 and 2010, and 370 since 2010 (2010-March 2018). Far from holding a monopoly over the million results to a search for “evolutionary biology,” “function-valued traits” are a mere drop in the bucket. The "Function-valued Traits Working Group," listed two major limitations in 2012 that have yet to be thoroughly addressed. Firstly, the cost of additional measurements required to estimate functions presents a challenge, and secondly a general unfamiliarity with function-valued analyses is limiting its uptake by scientists (Stinchcombe & Kirkpatrick, 2012). As a step towards solving the second challenge, this guide aims to serve as an introduction to function-valued traits, and function-valued trait analysis. This resource is designed for biology students with some experience in R and a basic understanding of statistics. We begin by diving headlong into examples and sorting through the theory as we go. At the end of this text there are recommended activities and example problems so that readers can acquaint themselves more fully with function-valued traits. 

## Examples

### Fitting a Function to Data

If you've ever hit "add trendline" and "display equation on chart" in excel then you've already done this... sort of. Here we will explore how to fit a function to data in R.

First, we need data. I have created a sample dataset comprised of the lengths of the mythical species of fish, R-ctic ch-R. This dataset has both random individual variance, and random measurement error, and details about how the data was generated have been outlined in the *Data Generation* document. Here is a plot of the data:

```{r chR, include=FALSE}
RcticChR <- function(a, k, g, x) {
  0.5+(a*exp(-exp(-k*(x-g)))-exp(-exp(-k*(-g))))
}
inp <- seq(from = 0.5, to = 10, by = 0.5)
a1 <- 1
k1 <- 1
g1 <- 3
RcticChR1 <- data.frame(matrix(NA, nrow=10, ncol=20))
Mono1akg <- data.frame(matrix(NA, nrow=100, ncol=3))
for (i in 1:10) {
  set.seed(1234*i)
  MonoError <- rnorm(n=20, mean=0, sd=0.04) #measurement error
  IndiDev <- rnorm(n=3, mean=0, sd=0.2) #the individual's deviation from the pop. av.
  for (l in 1:20){
    RcticChR1[i,l] <- RcticChR((a1+IndiDev[1]), (k1+IndiDev[2]), (g1+IndiDev[3]), (l/2)) + MonoError[l]
  }
  Mono1akg[i,1] <- a1+IndiDev[1]
  Mono1akg[i,2] <- k1+IndiDev[2]
  Mono1akg[i,3] <- g1+IndiDev[3]
}
```

```{r pressure, echo=FALSE, fig.cap=fig$cap("R-ctic Ch-R 1", "A plot of the ten male R-ctic Ch-R sampled.")}

#cols <- c("#000000", "#332288", "#88CCEE", "#44AA99", "#117733", "#999933", "#DDCC77", "#CC6677", "#882255", "#AA4499")

cols <- c("#000000", 2, 3, 4, 5, 6, 7, 8, "#882255", "darkolivegreen2")

matplot(y = t(RcticChR1), type = 'p', lty = 1, pch = 16, main = "Males", ylab = "Length (cm)", xlab = "Age (weeks)", las = 1, col = alpha(cols, 0.5))
```

Now that we have data we can fit various models to them. Let's begin by fitting a series of polynomials to the data in a process called polynmial regression. For simplicity, we will focus on one individual, from one population of R-ctic ch-R. Let's take the first male, `RcticChR1ind1`.

```{r, include=FALSE}
RcticChR1ind1 <- rep(NA, 20)
for (i in 1:20) {
  RcticChR1ind1[i] <- RcticChR1[1,i]
}
```

```{r, echo = FALSE, fig.cap=fig$cap("RcticChR1ind1", "A plot of the true and measured length of RcticChR1ind1.")}
xvalues <- seq(from = 0.5, to = 10, by = 0.5) #This object will come up often so take 
plot(y = RcticChR1ind1, x = xvalues, main = "RcticChR1ind1", ylab = "Length (cm)", xlab = "Age (weeks)", las = 1)
curve(RcticChR(Mono1akg[1,1], Mono1akg[1,2], Mono1akg[1,3], x), n = 101, add = TRUE, col = "red")
legend("bottomright",c("Measured Length", "True Length"), 
       col = c("black", "red"), pch = c(1, NA), lwd = c(NA, 1), lty = c(1, 1))

```


In order to fit our polynomial model to `RcticChR1ind1` we will use the linear models function in R. Unlike its name suggests, the "linear model" function in R can do far more than merely slap on the closest linear approximation! We can get it to fit a polynomial of whatever order we desire by specifying the highest exponent in the model. 

```{r}
#First we need to define the x-values,
#In this case they represent the points in time when scientists measured the lengths of the fish.
xvalues <- seq(from = 0.5, to = 10, by = 0.5) #This object will come up often so take note!
#Now let's create an object for this model
PolyModelPop1ind1 <- lm(RcticChR1ind1 ~ poly(x = xvalues, n = 6))
```

From this we can calculate the predicted 99% confidence intervals.

```{r}
predicted.intervals <- predict(PolyModelPop1ind1, data.frame(x=inp), interval='confidence', level=0.99)
```

Now that we have a model we can plot it against the data to see how it holds up. 

First let's plot the data.

```{r, eval=FALSE}
plot.new(y = RcticChR1ind1, x = xvalues, main = "Polynomial Regression", ylab = "Length (cm)", xlab = "Age (weeks)", las = 1)
```

Then add the secret function of the true length of RcticChR1ind1 used to generate the data.

```{r, eval=FALSE}
curve(RcticChR(Mono1akg[1,1], Mono1akg[1,2], Mono1akg[1,3], x), n = 101, add = TRUE, col = "red")
```

And we're ready to plot the approximated polynomial function.

```{r, eval=FALSE}
lines(inp,predicted.intervals[,1],col='green', lwd=3)
lines(inp,predicted.intervals[,2],col=4, lwd=1, lty=2)
lines(inp,predicted.intervals[,3],col=4, lwd=1, lty=2)
```

Throw in a legend and we're good to go.

```{r, eval=FALSE}
legend("bottomright",c("Measured Length", "True Length", "Predicted Length", "99% Confidence Interval"), 
       col = c("black", "red", "green", 4), pch = c(1, NA, NA, NA), lwd = c(NA, 1, 3, 1), lty = c(1, 1, 1, 2))
```

```{r, echo=FALSE, fig.cap=fig$cap("R-ctic Ch-R 1", "The polynomial fit as compares to the real function and the measured data.")}
plot(y = RcticChR1ind1, x = xvalues, main = "Polynomial Regression", ylab = "Length (cm)", xlab = "Age (weeks)", las = 1)
curve(RcticChR(Mono1akg[1,1], Mono1akg[1,2], Mono1akg[1,3], x), n = 101, add = TRUE, col = "red")
lines(inp,predicted.intervals[,1],col='green', lwd=3)
lines(inp,predicted.intervals[,2],col=4, lwd=1, lty=2)
lines(inp,predicted.intervals[,3],col=4, lwd=1, lty=2)
legend("bottomright",c("Measured Length", "True Length", "Predicted Length", "99% Confidence Intervals"), 
       col = c("black", "red", "green", 4), pch = c(1, NA, NA, NA), lwd = c(NA, 1, 3, 1), lty = c(1, 1, 1, 2))


```

This green line seems like a reasonable fit. But what if we hadn't chosen n = 6? How did that arbitrary decision impact the model? Let's take it to the max, n = 19, and see what happens.

```{r, echo=FALSE, fig.cap=fig$cap("R-ctic Ch-R 1", "An interactive plot demonstrating the change in shape of the polynimial fit as the number of terms in the approximating polynomial is adjusted.")}


PolyModelPop1ind1 <- lm(RcticChR1ind1 ~ poly(x = inp, n = 19))
predicted.intervals <- predict(PolyModelPop1ind1, data.frame(x=inp), interval='confidence', level=0.99)
  
  plot(y = RcticChR1ind1, x = xvalues, main = "Polynomial Regression", ylab = "Length (cm)", xlab = "Age (weeks)", las = 1)
  curve(RcticChR(Mono1akg[1,1], Mono1akg[1,2], Mono1akg[1,3], x), n = 101, add = TRUE, col = "red")
  lines(inp,predicted.intervals[,1],col='green', lwd=3)
  lines(inp,predicted.intervals[,2],col=4, lwd=1, lty=2)
  lines(inp,predicted.intervals[,3],col=4, lwd=1, lty=2)
  legend("bottomright",c("Measured Length", "True Length", "Predicted Length", "99% Confidence Intervals"), col = c("black", "red", "green", 4), pch = c(1, NA, NA, NA), lwd = c(NA, 1, 3, 1), lty = c(1, 1, 1, 2))


```

It is important to note what happpens when n = 19. Here the approximate function has been overparametrized to the extreme and has lost all predictive value. At n = 19 the model also loses statistical power because when the leading term is x^19^, there are 20 coefficients to analyze, the same as the number of datapoints. The best choice for n would be between 4 and 6.

### Analysis

Ok so we've got a model, but what is R actually doing? Let's see what is in this mysterious object "`PolyModelPop1ind1`".

```{r}
PolyModelPop1ind1
```

Evaluating `PolyModelPop1ind1` we see that it is composed of seven `Coefficients`.
Under the hood, R is adding together functions we told it to combine, namely, a costant, x, x^2^, x^3^, etc. up to x^6^. These `Coeffficients` correspond to the amplitudes of the functions making up the green curve shown in the figures above. In other words they tell us how much of each function is being added. Below is a visualization of the functions present when n = 6.

```{r, echo=FALSE, fig.cap=fig$cap("Coefficients", "A plot of the functions making up the polynomial approximation of RcticChR1ind1's growth function.")}
plot(1, type="n", xlab="", ylab="", xlim = c(0, 10), ylim = c(-50, 75))
curve(1.63443*x, xlim = c(-1, 11), ylim = c(-55, 55), add = TRUE, col = 1)
curve(-0.58209*x^2, to = 10, xlim = c(-1, 11), ylim = c(-55, 55), add = TRUE, col = 2)
curve(-0.07315*x^3, to = 10, xlim = c(-1, 11), ylim = c(-55, 55), add = TRUE, col = 3)
curve(0.26519*x^4, to = 10, xlim = c(-1, 11), ylim = c(-55, 55), add = TRUE, col = 4)
curve(-0.02839*x^5, to = 10, xlim = c(-1, 11), ylim = c(-55, 55), add = TRUE, col = 5)
curve(0.03007*x^6, to = 10, xlim = c(-1, 11), ylim = c(-55, 55), add = TRUE, col = 6)
abline(h=0.67789,col= "orange")
legend("topright", c("Constant", "x", as.expression(bquote( ~ x^2 ~ "")), as.expression(bquote( ~ x^3 ~ "")), as.expression(bquote( ~ x^4 ~ "")), as.expression(bquote( ~ x^5 ~ "")), as.expression(bquote( ~ x^6 ~ ""))), col = c("orange", 1, 2, 3, 4, 5, 6), lty = c(1, 1, 1, 1, 1))
```

Many of these functions get very big, very quickly, but when added together they make up a fair approximation of the length of `RcticChR1ind1`.

Now that we have gotten R to describe the model function it's come up with, we are finally ready to broaden our scope from our friend 'RcticChR1ind1' and consider theis entire sample. By repeating the process outlined above for all of the samples of the lengths of male Rctic ChR, we can collect a dataset of coefficients.

```{r}
#Take the data frame of the sample lengths
str(RcticChR1)

#and make an empty data frame for the coefficients
PolyModel1 <- data.frame(matrix(data=NA,nrow=7,ncol=10))

#Now we can fill the empty data frame with the coefficients from the RcticChR1 sample
for (l in 1:10) {
  PolyModel1[l] <- coefficients(lm(as.matrix(RcticChR1)[l,] ~ poly(x = xvalues, n = 6)))
}

#Let's take a look at what we've got
str(PolyModel1)
```

Here we see that the data frame of coefficients is much more concise than the data frame of the data, but this does not mean that information is lost. (Note that where X10 marked the 10th measurement in the data frame `RcticChR1`, it now marks the 10th individual in `PolyModel1`).

Now let's run an analysis between this sample and that of the female Rctic ChR.
```{r, include=FALSE}
Mono1ind <- matrix(data=NA,nrow=10,ncol=20)

for (l in 1:10) {
  for (i in 1:20) {
    Mono1ind[l,i] <- RcticChR1[l,i]
  }
}

a2 <- 1.4
k2 <- 1
g2 <- 2.6

#For reference:
#population 1:
#a1 <- 1
#k1 <- 1
#g1 <- 3

#Once again we need to build a data frame for our data
RcticChR2 <- data.frame(matrix(NA, nrow=10, ncol=20))

#and one to keep track of what a, k, and g are for each individual in this population
Mono2akg <- data.frame(matrix(NA, nrow=10, ncol=3))


#Time to fill these frames
for (i in 1:10) {
  set.seed(1234*i)
  MonoError <- rnorm(n=20, mean=0, sd=0.04) #measurement error
  IndiDev <- rnorm(n=3, mean=0, sd=0.2) #individual deviation from population mean
  for (l in 1:20){
    RcticChR2[i,l] <- RcticChR(a2+IndiDev[1], k2+IndiDev[2], g2+IndiDev[3], (l/2)) + MonoError[l]
  }
  Mono2akg[i,1] <- a2+IndiDev[1]
  Mono2akg[i,2] <- k2+IndiDev[2]
  Mono2akg[i,3] <- g2+IndiDev[3]
}
```

```{r}
#Once again let's make an empty data frame for our coefficients
PolyModel2 <- data.frame(matrix(data=NA,nrow=7,ncol=10))

#and fill the empty data frame with the coefficients from the RcticChR2 sample
for (l in 1:10) {
  PolyModel2[l] <- coefficients(lm(as.matrix(RcticChR2)[l,] ~ poly(x = xvalues, n = 6)))
}

#Let's check this to make sure it worked
str(PolyModel2)
```

Before we go any further we will need to install a package in order to be able to run a Hotellings T^2^ test.

```{r, include=FALSE}
library("ICSNP")
```

```{r, eval=FALSE}
library("ICSNP")
```

Now we are ready to run the test!

```{r}
HotellingsT2(PolyModel1, PolyModel2)
```


According to this result we have failed to reject the null hypothesis that the sizes of female and male populations of Rctic ChR differ significantly. Before we conclude let us plot these two samples together to see whether or not this is a reasonable outocome.

```{r, fig.cap=fig$cap("Male vs. Female", "A comparison of male and female Rctic ChR lengths")}
matplot(y = t(RcticChR1), type = 'p', lty = 1, pch = 10, ylab = "Length (cm)", xlab = "Age (weeks)", las = 1, col = 1, ylim = c(0, 2.5))
matplot(y = t(RcticChR2), type = 'p', lty = 1, pch = 1, las = 1, col = 2, add = TRUE)
legend("bottomright",c("Males", "Females"), col = c("black", "red"), pch = c(10, 1))
```

There is a significant amount of overlap between these measurements of males and females which sipports our calculation that we cannot reject the null hypothesis.

### Fitting a Sinusoidal Function to Data (still in the works)

Okay so we fit a polynomial to the data, but is that really the best fit? In e]Excel the user can choose from a selection of fits to suit their data. Let's not be outdone by a comouter program! Here we will go beyond the capabilities of Excel and fit a sinusoidal model to the data.

To begin, we can use the function `spectrum` to get the spectral density of `RcticChR1ind1`.

```{r}
spctrm <- spectrum(RcticChR1ind1)
```

This will tell us the prominence of various frequencie in the data. If there are consistently peaks at a set interval, then that will feature prominently in this spectrum as an abundant frequency. From this information we can extract the period of the most prominent frequency by knowing that the period is the reciprocal of the frequency.

```{r}
period <- 1/spctrm$freq[spctrm$spec==max(spctrm$spec)]
```

Now we're ready to make a model. Even though we are now even further from a linear model than we were with the polynomial approximation, we are going to use the linear model function once again.

```{r}
SinModelPop1ind1 <- lm(RcticChR1ind1 ~ sin(2*pi/period*xvalues)+cos(2*pi/period*xvalues))
```

Again, let's find the predicted intervals.

```{r}
PredictedIntervalsSin <- predict(SinModelPop1ind1, data.frame(x = xvalues),interval='confidence', level=0.99)
```

Now we're ready to plot our model. We can start by plotting the data and true function as before.

```{r, tidy=TRUE, eval=FALSE}
plot(y = RcticChR1ind1, x = xvalues, main = "Polynomial Regression", ylab = "Length (cm)", xlab = "Age (weeks)", las = 1)
curve(RcticChR(Mono1akg[1,1], Mono1akg[1,2], Mono1akg[1,3], x), n = 101, add = TRUE, col = "red")
```

And then add the approximate function and a legend.

```{r, tidy=TRUE, eval=FALSE}
lines(fitted(SinModelPop1ind1)~xvalues, col='green', lwd=3)


lines(xvalues, PredictedIntervalsSin[,2],col=4, lwd=1, lty=2)
lines(xvalues, PredictedIntervalsSin[,3],col=4, lwd=1, lty=2)

legend("bottomright",c("Measured Length", "True length", "Predicted length", "99% Confidence Interval"), col = c("black", "red", "green", 4), pch = c(1, NA, NA, NA), lwd = c(NA, 1, 3, 1), lty = c(1, 1, 1, 2))

```

```{r, echo=FALSE}
plot(y = RcticChR1ind1, x = xvalues, main = "Sinusoidal Regression", ylab = "Length (cm)", xlab = "Age (weeks)", las = 1)
curve(RcticChR(Mono1akg[1,1], Mono1akg[1,2], Mono1akg[1,3], x), n = 101, add = TRUE, col = "red")
lines(fitted(SinModelPop1ind1)~xvalues, col='green', lwd=3)
lines(xvalues, PredictedIntervalsSin[,2],col=4, lwd=1, lty=2)
lines(xvalues, PredictedIntervalsSin[,3],col=4, lwd=1, lty=2)
legend("bottomright",c("Measured Length", "True Length", "Predicted Length", "99% Confidence Interval"), col = c("black", "red", "green", 4), pch = c(1, NA, NA, NA), lwd = c(NA, 1, 3, 1), lty = c(1, 1, 1, 2))

```

This fit is comparable to a third degree polynomial which isn't so great. Let's see if we can improve the fit by adding a second harmonic.

```{r, include=FALSE}
SinModel2Pop1ind1 <- lm(RcticChR1ind1 ~ sin(2*pi/period*xvalues)+cos(2*pi/period*xvalues)+sin(4*pi/period*xvalues)+cos(4*pi/period*xvalues))

PredictedIntervalsSin2 <- predict(SinModel2Pop1ind1, data.frame(x = xvalues),interval='confidence', level=0.99)
```

Now let's plot this and see how our approximation improves.

```{r}
plot(y = RcticChR1ind1, x = xvalues, main = "Sinusoidal Regression", ylab = "Length (cm)", xlab = "Age (weeks)", las = 1)
curve(RcticChR(Mono1akg[1,1], Mono1akg[1,2], Mono1akg[1,3], x), n = 101, add = TRUE, col = "red")
lines(fitted(SinModelPop1ind1)~xvalues, col="black", lwd=1)
lines(fitted(SinModel2Pop1ind1)~xvalues, col='green', lwd=3)
lines(xvalues, PredictedIntervalsSin2[,2],col=4, lwd=1, lty=2)
lines(xvalues, PredictedIntervalsSin2[,3],col=4, lwd=1, lty=2)
legend("bottomright",c("Measured Length", "True Length", "First Harmonic", "Second Harmonic", "99% Confidence Interval"), col = c("black", "red", "black", "green", 4), pch = c(1, NA, NA, NA, NA), lwd = c(NA, 1, 1, 3, 1), lty = c(1, 1, 1, 1, 2))
```

###Under the Hood

Now just as we did with the polynomial regression we can evaluate this model and take a look at the inner workings.

```{r, eval=FALSE}
SinModel2Pop1ind1
```

Even though we are no longer dealing with polynomials, this call still returns `Coefficients`. Just as before these are the amplitudes of various functions making up the model. Below is a plot of the functions that are being added together to form the model.

```{r, echo=FALSE, eval=FALSE}
plot(1, type="n", xlab="", ylab="", xlim = c(0, 10), ylim = c(-2.0, 1.5))
curve(-0.3638*sin(2 * pi/period * x), xlim = c(-1, 11), ylim = c(-2, 2), add = TRUE, col = 1)
curve(-0.5538*cos(2 * pi/period * x), xlim = c(-1, 11), ylim = c(-2, 2), add = TRUE, col = 2)
curve(0.0227*sin(4 * pi/period * x), xlim = c(-1, 11), ylim = c(-2, 2), add = TRUE, col = 3)
curve(-0.3423 *cos(4 * pi/period * x), xlim = c(-1, 11), ylim = c(-2, 2), add = TRUE, col = 4)
abline(h=0.8813,col= "orange")
```

## Activities
###Exercises:
1. Manipulate the following function, adding/removing terms, changing coefficients etc. and plot the results to gain an intuition for polynomials.
```{r}
polynomial <- function(x) {
  0 + x + 2*x^2 + 3*x^3 +4*x^4 +5*x^5 + 6*x^6
}
```

###Practice Problems:
1. a) Open the sample dataset CatTemp1.csv representing the growth rate of Freija Fritillary caterpillars as a function of temperature and write a program that fits a polynomial to the data. (Growth rate (mg day^–1^))
      b) Use your program to fit a polynomial to the data in CatTemp2.csv representing the growth rate of the Stella Orange Tip as a function of temperature and compare these populations using multivariate and function-valued methods.
      c) Plot the data of both populations, and their polynomial approximations on the same graph. Does it look like there ought to be a 
    d) TroubleShooting: Iport the data into Excel and use the plotting function to fit a polynomial of bestfit to the data. Do Excel's coefficients match those you found in R?
    
2. Determine whether or not there is a significant diference between the datasets:
(Still need to get it to export nicely but I want this to be the cyclic functions that I have defned, and I want erratic measurements to make it difficult to test a significant difference between the populations without using FVT analysis).