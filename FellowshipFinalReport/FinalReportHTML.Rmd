---
title: "Introduction to Function-Valued Analysis"
author: "Anika Watson"
date: '2018-01-16'
runtime: shiny
output:
  html_document:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r functions, include=FALSE}
# A function for captioning and referencing images
fig <- local({
    i <- 0
    ref <- list()
    list(
        cap=function(refName, text) {
            i <<- i + 1
            ref[[refName]] <<- i
            paste("Figure ", i, ": ", text, sep="")
        },
        ref=function(refName) {
            ref[[refName]]
        })
})
``` 

```{r, include=FALSE}
#opsin labels and Lambda_max values for their sensitivity curves
opsins <- c("short wavelength", "long wavelenght")
lmax <- c(480., 620.)


nOpsin <- length(opsins) #number of opsins

nCoeff <- 2  #number of coefficients in Hermite series expansion

#make a vector of independent biological functions of the visual system that are 
#assumed to be the target of selection
functions <- c("foton capture efficiency", "peak sensitivity", "sensitivity range")
nFunc <- length(functions)


la <- 200
lb <- 800

#here "x" replaces the lowercase lambda in the Mathematica code,
#and "L" replaces the capilal lambda
Aband1 <- function(x,L) {
  a <- (0.8795 + 0.0459*exp(-(L - 300.)^(2/11940.)))
  1.0/(exp(69.7*(a - (L/x))) + exp(28.*(0.922 - (L/x))) + 
         exp(-14.9*(1.104 - (L/x))) + 0.674)
}

#as before, "x" replaces the lowercase lambda in the Mathematica code,
#and "L" replaces the capilal lambda, 
#also, "m" replaces mu and b replaces beta
Bband1 <- function(x,L) {
  m <- 189. + 0.315*L
  b <- -40.5 + 0.195*L
  0.26*exp(-((x - m)/b)^2)
}
```


```{r, include=FALSE}
library("scales")
library(shiny)
```

## Introduction
##Throwing out the data with the bathwater. 

Despite our best attempts to conduct thorough studies, many biologists are unwittingly throwing away valuable data on a regular basis. One major cause for this loss is that many biological traits are often described by single measurements when they are, in fact, too complex for such a simple framework. Consider, for instance, data for the spectrum of light absorbed by an eye (see Figure 1). These spectra are often distilled down to a single number, such as the peak wavelength (the wavelength of light that the eye is best tuned to see) (eg. Thor et al., 2016). This simplification ignores much of the data, and limits our ability to quantify the relationship between light wavelength absorbance, and, for instance, the spectrum of ambient light present. Treating these traits as functions, rather than single data points, has many statistical advantages including increased statistical power and others, which we will discuss shortly (Stinchcombe et al., 2012; Griswoldet al., 2008). Unfortunately, due to its perceived mathematical sophistication biologists have been hesitant to adopt function-valued methods. The good news is that even relatively simple function-valued analyses can provide valuable insight compared to traditional approaches (Griswold et al., 2008; Baker et al., 2015). This guide aims to serve as an introduction to function-valued trait analysis providing thorough examples including R code to dispel fear of this topic.

```{r, echo=FALSE}

#Let's plot the opsin spcrta as a combinations of Aband1 and Bband1
curve(Aband1(x, lmax[1]) + Bband1(x, lmax[1]), la, lb, 101, col = "red", las=1, ylab="Normalized Absorbance", xlab="Wavelength (nm)")
par(new=TRUE)
curve(Aband1(x, lmax[2]) + Bband1(x, lmax[2]), la, lb, 101, ylab = "", xlab = "", col = "blue", axes = FALSE)
legend(199, 1, legend = opsins, col=c("red", "blue"), lty=1, cex=0.8)

```

This approach first appeared in literature as far back as 1989 when Mark Kirkpatrick and David Lofsvold noticed that many evolutionarily important traits could be described as functions (Kirkpatrick & Lofsvol, 1989). This caught the eyes of breeders, biologists and statisticians gaining support from various fields as scientists realized just how widespread function-valued traits are. “A function-valued trait is any trait that changes in response to another variable” (Stinchcombe et al., 2012), so given that this includes any traits that change with respect to time, gene expression, or environmental conditions there is no shortage of examples. In a 1998 article W.G. Hill predicted that function-valued methods may eventually replace traditional methods in evolutionary biology (Griswold et al., 2008). The term "function-valued traits" was coined the very next year (Pletcher and Geyer, 1999). 

According to a paper by the "Function-valued Traits Working Group," the function-valued perspective receives such praise because it "offers enhanced statistical power, greater ability to detect genetic constraints, and improved understanding of phenotypic and genetic variation in environmentally sensitive traits" compared to traditional multivariate methods (Stinchcombe & Kirkpatrick, 2012). These advantages come from the fact that fitting a function to data takes into account the ordering and spacing of the data where multivariable methods ignore this information (Griswold et al., 2008), and because fitting a curve to data smooths noise, reducing its effects on analysis (Stinchcombe et al., 2012). Another advantage of function-valued analysis is that it does not care which points the researcher selected to sample their data. One can just as easily fit a curve estimating the growth of a plant if its height is measured on odd days, as if its height were to be measured on even days. This allows for comparison between data sets with different numbers of samples, and whose samples were taken at different points.

Despite these clear advantages, and Hill's prediction, function-valued trait analysis still hasn't taken off. A quick online search for "function-valued traits" returns 7 results between 1990 and 2000, 236 results between 2000 and 2010, and 370 since 2010 (2010-March 2018). Far from holding a monopoly over the thousands of evolutionary biology, papers published each year. The "Function-valued Traits Working Group," listed two major limitations in 2012 that have yet to be thoroughly addressed. Firstly, the cost of additional measurements required to estimate functions presents a challenge, and secondly a general unfamiliarity with function-valued analyses is limiting its uptake by scientists (Stinchcombe & Kirkpatrick, 2012). As a step towards solving the second challenge, this guide aims to serve as an introduction to function-valued traits, and function-valued trait analysis. This resource is designed for biologists with some experience in R and a basic understanding of statistics. We begin by diving headlong into examples and sorting through the theory as we go. At the end of this text there are recommended activities and example problems so that readers can acquaint themselves more fully with function-valued traits. 

## Examples

### Fitting a Function to Data

If you've ever hit "add trendline" and "display equation on chart" in excel then you've already done this... sort of. Figure 2 shows data from a lab in a first year biology course in which students measure the volume of CO~2~ produced through yeast metabolism, and use this data to calculate the rate of metabolism. This is, at its foundation, the very simplest example of function-valued analysis.


```{r, echo=FALSE, fig.cap=fig$cap("Yeast Metabolism", "A plot of yeast metabolism data representing the volume of CO~2~ produced over time. Error bars represent standard error from three trials.")}
RcticG <- function(a, k, g, x) {
  0.75+10*(a*exp(-exp(-k*(x-g)))-exp(-exp(-k*(-g))))
}
Time <- seq(from = 1, to = 20, by = 1)
inp <- seq(from = 1, to = 20, by = 1)
a1 <- 1
k1 <- 0.31
g1 <- 10
Length <- data.frame(matrix(NA, nrow=1, ncol=20))
Mono1akg <- data.frame(matrix(NA, nrow=100, ncol=3))
set.seed(1235)
MonoError <- rnorm(n=20, mean=0, sd=0.4) #measurement error
for (l in 1:20){
  Length[1,l] <- RcticG(a1, k1, g1, (l)) + MonoError[l]
}
a2 <- 1.2
k2 <- 0.19
g2 <- 1
RcticG2 <- data.frame(matrix(NA, nrow=1, ncol=20))
Mono2akg <- data.frame(matrix(NA, nrow=100, ncol=3))
set.seed(1234)
MonoError <- rnorm(n=20, mean=0, sd=0.24) #measurement error
for (l in 1:20){
  RcticG2[1,l] <- RcticG(a2, k2, g2, (l)) + MonoError[l]
}

serr <- as.vector(c(0.55, 0.575, 0.6, 0.625, 0.65, 0.7, 0.75, 0.7275, 0.705, 0.3775, 0.05, 0.325, 0.6, 0.7, 0.8, 0.75, 0.76, 0.6, 0.78, 0.8))

serr2 <- as.vector(c(0.6, 0.570412415, 0.340824829, 0.390824829, 0.240824829, 0.390824829, 0.240824829, 0.390824829, 0.240824829, 0.470412415, 0.4, 0.470412415, 0.240824829, 0.411237244, 0.281649658, 0.472474487, 0.363299316, 0.554124145, 0.644948974))

YeastModel1 <- lm(as.numeric(as.vector(Length[1,])) ~ poly(x = Time, n = 1))

predicted.intervals1 <- predict(YeastModel1, data.frame(x=Time), interval='confidence', level=0.99)

YeastModel2 <- lm(as.numeric(as.vector(RcticG2[1,])) ~ poly(x = Time, n = 1))

predicted.intervals2 <- predict(YeastModel2, data.frame(x=Time), interval='confidence', level=0.99)

plot(y = Length, x = Time, ylim = c(0, 12), xlim = c(0, 20), ylab = expression('Volume CO'[2]* ' (mL)'), xlab = "Time (min)", las = 1)
points(y = RcticG2, x = Time, pch = 16, col = 3)

arrows(Time, as.numeric(as.vector(Length[1,]))-serr, Time, as.numeric(as.vector(Length[1,]))+serr, length=0.05, angle=90, code=3)

arrows(Time, as.numeric(as.vector(RcticG2[1,]))-serr2, Time, as.numeric(as.vector(RcticG2[1,]))+serr2, length=0.05, angle=90, code=3)

lines(inp,predicted.intervals1[,1],col='black', lwd=1, xlim=c(0,10))

lines(inp,predicted.intervals2[,1],col='black', lwd=1, xlim=c(0,10))

legend("bottomright",c("Quick-Rise Yeast", "Traditional Yeast"), 
       col = c(3, "black"), pch = c(16, 1))
```

It is clear from the graph, however, that these stright lines of bestfit do not represent the data very well, but without knowledge of the underlying function (is the growth exponential? logarithmic?), we may not want to guess at a better function to fit. Fortunately we don't have to. Here I will introduce a non-parametric method of function-valued traits that does not assume any particular function.

In order to do so, I have created a sample dataset comprised of the lengths of the mythical species of fish, R-ctic Grayling. This dataset has both random individual variance, and random measurement error, and details about how the data was generated have been outlined in the *Data Generation* document. Here is a plot of the data:

```{r Grayling, include=FALSE}
RcticG <- function(a, k, g, x) {
  0.5+(a*exp(-exp(-k*(x-g)))-exp(-exp(-k*(-g))))
}
inp <- seq(from = 0.5, to = 10, by = 0.5)
a1 <- 1
k1 <- 1
g1 <- 3
RcticG1 <- data.frame(matrix(NA, nrow=10, ncol=20))
Mono1akg <- data.frame(matrix(NA, nrow=100, ncol=3))
for (i in 1:10) {
  set.seed(1234*i)
  MonoError <- rnorm(n=20, mean=0, sd=0.04) #measurement error
  IndiDev <- rnorm(n=3, mean=0, sd=0.2) #the individual's deviation from the pop. av.
  for (l in 1:20){
    RcticG1[i,l] <- RcticG((a1+IndiDev[1]), (k1+IndiDev[2]), (g1+IndiDev[3]), (l/2)) + MonoError[l]
  }
  Mono1akg[i,1] <- a1+IndiDev[1]
  Mono1akg[i,2] <- k1+IndiDev[2]
  Mono1akg[i,3] <- g1+IndiDev[3]
}
```

```{r pressure, echo=FALSE, fig.cap=fig$cap("R-ctic Grayling 1", "A plot of the ten R-ctic Grayling sampled from population 1.")}

#cols <- c("#000000", "#332288", "#88CCEE", "#44AA99", "#117733", "#999933", "#DDCC77", "#CC6677", "#882255", "#AA4499")

cols <- c("#000000", 2, 3, 4, 5, 6, 7, 8, "#882255", "darkolivegreen2")

matplot(y = t(RcticG1), type = 'p', lty = 1, pch = 16, main = NULL, ylab = "Length (feet)", xlab = "Age (weeks)", las = 1, col = alpha(cols, 0.5))
```

Now that we have data we can fit various models to them. Let's begin by fitting a series of polynomials to the data in a process called polynmial regression. For simplicity, we will focus on one individual, from one population of R-ctic Grayling. Let's take the first individual from the first population, `RcticG1ind1`.

```{r, include=FALSE}
RcticG1ind1 <- rep(NA, 20)
for (i in 1:20) {
  RcticG1ind1[i] <- RcticG1[1,i]
}
```

```{r, echo = FALSE, fig.cap=fig$cap("RcticG1ind1", "A plot of the true and measured length of `RcticG1ind1`.")}
xvalues <- seq(from = 0.5, to = 10, by = 0.5) #This object will come up often so take 
plot(y = RcticG1ind1, x = xvalues, main = NULL, ylab = "Length (feet)", xlab = "Age (weeks)", las = 1)
curve(RcticG(Mono1akg[1,1], Mono1akg[1,2], Mono1akg[1,3], x), n = 101, add = TRUE, col = "red")
legend("bottomright",c("Measured Length", "True Length"), 
       col = c("black", "red"), pch = c(1, NA), lwd = c(NA, 1), lty = c(1, 1))

```


In order to fit our polynomial model to `RcticG1ind1` we will use the linear models function in R. Unlike its name suggests, the "linear model" function in R can do far more than merely slap on the closest straight line approximation! We can get it to fit a polynomial of whatever order we desire by specifying the highest exponent in the model. 

```{r}
#First we need to define the x-values,
#In this case they represent the points in time when scientists measured the lengths of the fish.
xvalues <- seq(from = 0.5, to = 10, by = 0.5) #This object will come up often so take note!
#Now let's create an object and assign the model to it
PolyModelPop1ind1 <- lm(RcticG1ind1 ~ poly(x = xvalues, n = 6))
```

From this we can calculate the predicted 99% confidence intervals.

```{r}
predicted.intervals <- predict(PolyModelPop1ind1, data.frame(x=inp), interval='confidence', level=0.99)
```

Now that we have a model we can plot it against the data to see how it holds up. 

First let's plot the data.

```{r, eval=FALSE}
plot.new(y = RcticG1ind1, x = xvalues, main = "Polynomial Regression", ylab = "Length (feet)", xlab = "Age (weeks)", las = 1)
```

Then add the secret function of the true length of RcticG1ind1 used to generate the data.

```{r, eval=FALSE}
curve(RcticG(Mono1akg[1,1], Mono1akg[1,2], Mono1akg[1,3], x), n = 101, add = TRUE, col = "red")
```

And we're ready to plot the approximated polynomial function.

```{r, eval=FALSE}
lines(inp,predicted.intervals[,1],col='green', lwd=3)
lines(inp,predicted.intervals[,2],col=4, lwd=1, lty=2)
lines(inp,predicted.intervals[,3],col=4, lwd=1, lty=2)
```

Throw in a legend and we're good to go.

```{r, eval=FALSE}
legend("bottomright",c("Measured Length", "True Length", "Predicted Length", "99% Confidence Interval"), 
       col = c("black", "red", "green", 4), pch = c(1, NA, NA, NA), lwd = c(NA, 1, 3, 1), lty = c(1, 1, 1, 2))
```

```{r, echo=FALSE, fig.cap=fig$cap("R-ctic Grayling 1", "The polynomial fit as compares to the real function and the measured data.")}
plot(y = RcticG1ind1, x = xvalues, main = NULL, ylab = "Length (feet)", xlab = "Age (weeks)", las = 1)
curve(RcticG(Mono1akg[1,1], Mono1akg[1,2], Mono1akg[1,3], x), n = 101, add = TRUE, col = "red")
lines(inp,predicted.intervals[,1],col='green', lwd=3)
lines(inp,predicted.intervals[,2],col=4, lwd=1, lty=2)
lines(inp,predicted.intervals[,3],col=4, lwd=1, lty=2)
legend("bottomright",c("Measured Length", "True Length", "Predicted Length", "99% Confidence Intervals"), 
       col = c("black", "red", "green", 4), pch = c(1, NA, NA, NA), lwd = c(NA, 1, 3, 1), lty = c(1, 1, 1, 2))


```

This green line seems like a reasonable fit. But what if we hadn't chosen n=6? How did that arbitrary decision impact the model? Slide the toggle below to see for yourself.

```{r, echo=FALSE}
sliderInput("numb", label = "Polynomial Degree", min = 1, max = 19, value = 6, step = 1, animate = T)
```


```{r, echo=FALSE, fig.cap=fig$cap("R-ctic Grayling 1", "An interactive plot demonstrating the change in shape of the polynimial fit as the number of terms in the approximating polynomial is adjusted.")}

renderPlot({
  PolyModelPop1ind1 <- lm(RcticG1ind1 ~ poly(x = inp, n = input$numb))
  predicted.intervals <- predict(PolyModelPop1ind1, data.frame(x=inp), interval='confidence', level=0.99)
  
  plot(y = RcticG1ind1, x = xvalues, main = NULL, ylab = "Length (feet)", xlab = "Age (weeks)", las = 1)
  curve(RcticG(Mono1akg[1,1], Mono1akg[1,2], Mono1akg[1,3], x), n = 101, add = TRUE, col = "red")
  lines(inp,predicted.intervals[,1],col='green', lwd=3)
  lines(inp,predicted.intervals[,2],col=4, lwd=1, lty=2)
  lines(inp,predicted.intervals[,3],col=4, lwd=1, lty=2)
  legend("bottomright",c("Measured Length", "True Length", "Predicted Length", "99% Confidence Intervals"), col = c("black", "red", "green", 4), pch = c(1, NA, NA, NA), lwd = c(NA, 1, 3, 1), lty = c(1, 1, 1, 2))
})

```

It is important to note what happpens when n = 19. Here the approximate function has been overparametrized to the extreme and has lost all predictive value. At n = 19 the model also loses statistical power because when the leading term is x^19^, there are 20 coefficients to analyze, the same as the number of datapoints. The best choice for n would be between 4 and 6.

### Analysis

Ok so we've got a model, but what is R actually doing? Let's see what is in this mysterious object "`PolyModelPop1ind1`".

```{r}
PolyModelPop1ind1
```

Evaluating `PolyModelPop1ind1` we see that it is composed of seven `Coefficients`.
Under the hood, R is adding together functions we told it to combine, namely, a costant, x, x^2^, x^3^, etc. up to x^6^. These `Coeffficients` correspond to the amplitudes of the functions making up the green curve shown in the figures above. In other words they tell us how much of each function is being added. Below is a visualization of the functions present when n = 6.

```{r, echo=FALSE, fig.cap=fig$cap("Coefficients", "A plot of the functions making up the polynomial approximation of `RcticG1ind1`'s growth function.")}
plot(1, type="n", xlab="", ylab="", xlim = c(0, 10), ylim = c(-50, 100))
curve(1.63443*x, xlim = c(-1, 11), ylim = c(-55, 55), add = TRUE, col = 1)
curve(-0.58209*x^2, to = 10, xlim = c(-1, 11), ylim = c(-55, 55), add = TRUE, col = 2)
curve(-0.07315*x^3, to = 10, xlim = c(-1, 11), ylim = c(-55, 55), add = TRUE, col = 3)
curve(0.26519*x^4, to = 10, xlim = c(-1, 11), ylim = c(-55, 55), add = TRUE, col = 4)
curve(-0.02839*x^5, to = 10, xlim = c(-1, 11), ylim = c(-55, 55), add = TRUE, col = 5)
curve(0.03007*x^6, to = 10, xlim = c(-1, 11), ylim = c(-55, 55), add = TRUE, col = 6)
abline(h=0.67789,col= "orange")
legend("topright", c("Constant", "x", as.expression(bquote( ~ x^2 ~ "")), as.expression(bquote( ~ x^3 ~ "")), as.expression(bquote( ~ x^4 ~ "")), as.expression(bquote( ~ x^5 ~ "")), as.expression(bquote( ~ x^6 ~ ""))), col = c("orange", 1, 2, 3, 4, 5, 6), lty = c(1, 1, 1, 1, 1))
```

Many of these functions get very big, very quickly, but when added together they make up a fair approximation of the length of `RcticG1ind1`.

Now that we have gotten R to describe the model function it's come up with, we are finally ready to broaden our scope from our friend 'RcticG1ind1' and consider theis entire sample. By repeating the process outlined above for all of the samples of the lengths of male R-ctic Grayling, we can collect a dataset of coefficients.

```{r}
#Take the data frame of the sample lengths
str(RcticG1)

#and make an empty data frame for the coefficients
PolyModel1 <- data.frame(matrix(data=NA,nrow=7,ncol=10))

#Now we can fill the empty data frame with the coefficients from the RcticG1 sample
for (l in 1:10) {
  PolyModel1[l] <- coefficients(lm(as.matrix(RcticG1)[l,] ~ poly(x = xvalues, n = 6)))
}

#Let's take a look at what we've got
str(PolyModel1)
```

Here we see that the data frame of coefficients is much more concise than the data frame of the data, but this does not mean that information is lost. (Note that where X10 marked the 10th measurement in the data frame `RcticG1`, it now marks the 10th individual in `PolyModel1`).

Now let's run an analysis between this sample and that of the female R-ctic Grayling.
```{r, include=FALSE}
Mono1ind <- matrix(data=NA,nrow=10,ncol=20)

for (l in 1:10) {
  for (i in 1:20) {
    Mono1ind[l,i] <- RcticG1[l,i]
  }
}

a2 <- 1.4
k2 <- 1
g2 <- 2.6

#For reference:
#population 1:
#a1 <- 1
#k1 <- 1
#g1 <- 3

#Once again we need to build a data frame for our data
RcticG2 <- data.frame(matrix(NA, nrow=10, ncol=20))

#and one to keep track of what a, k, and g are for each individual in this population
Mono2akg <- data.frame(matrix(NA, nrow=10, ncol=3))


#Time to fill these frames
for (i in 1:10) {
  set.seed(1234*i)
  MonoError <- rnorm(n=20, mean=0, sd=0.04) #measurement error
  IndiDev <- rnorm(n=3, mean=0, sd=0.2) #individual deviation from population mean
  for (l in 1:20){
    RcticG2[i,l] <- RcticG(a2+IndiDev[1], k2+IndiDev[2], g2+IndiDev[3], (l/2)) + MonoError[l]
  }
  Mono2akg[i,1] <- a2+IndiDev[1]
  Mono2akg[i,2] <- k2+IndiDev[2]
  Mono2akg[i,3] <- g2+IndiDev[3]
}
```

```{r}
#Once again let's make an empty data frame for our coefficients
PolyModel2 <- data.frame(matrix(data=NA,nrow=7,ncol=10))

#and fill the empty data frame with the coefficients from the RcticG2 sample
for (l in 1:10) {
  PolyModel2[l] <- coefficients(lm(as.matrix(RcticG2)[l,] ~ poly(x = xvalues, n = 6)))
}

#Let's check this to make sure it worked
str(PolyModel2)
```

Before we go any further we will need to install a package in order to be able to run a Hotellings T^2^ test.

```{r, include=FALSE}
library("ICSNP")
```

```{r, eval=FALSE}
library("ICSNP")
```

Now we are ready to run the test!

```{r}
HotellingsT2(PolyModel1, PolyModel2)
```


According to this result we have failed to reject the null hypothesis that the sizes of female and male populations of R-ctic Grayling differ significantly. Before we conclude let us plot these two samples together to see whether or not this is a reasonable outocome.

```{r, fig.cap=fig$cap("Male vs. Female", "A comparison of male and female R-ctic Grayling lengths")}
matplot(y = t(RcticG1), type = 'p', lty = 1, pch = 10, ylab = "Length (feet)", xlab = "Age (weeks)", las = 1, col = 1, ylim = c(0, 2.5))
matplot(y = t(RcticG2), type = 'p', lty = 1, pch = 1, las = 1, col = 2, add = TRUE)
legend("bottomright",c("Males", "Females"), col = c("black", "red"), pch = c(10, 1))
```

There is a significant amount of overlap between these measurements of males and females which sipports our calculation that we cannot reject the null hypothesis.

### Fitting a Sinusoidal Function to Data (still in the works)

Okay so we fit a polynomial to the data, but is that really the best fit? In Excel the user can choose from a selection of fits to suit their data. Let's not be outdone by a comouter program! Here we will go beyond the capabilities of Excel and fit a sinusoidal model to the data.

To begin, we can use the function `spectrum` to get the spectral density of `RcticG1ind1`.

```{r, fig.show= 'hide'}
spctrm <- spectrum(RcticG1ind1)
```

This will tell us the prominence of various frequencie in the data. If there are consistently peaks at a set interval, then that will feature prominently in this spectrum as an abundant frequency. From this information we can extract the period of the most prominent frequency by knowing that the period is the reciprocal of the frequency.

```{r}
period <- 1/spctrm$freq[spctrm$spec==max(spctrm$spec)]
```

Now we're ready to make a model. Even though we are now even further from a linear model than we were with the polynomial approximation, we are going to use the linear model function once again.

```{r}
SinModelPop1ind1 <- lm(RcticG1ind1 ~ sin(2*pi/period*xvalues)+cos(2*pi/period*xvalues))
```

Again, let's find the predicted intervals.

```{r}
PredictedIntervalsSin <- predict(SinModelPop1ind1, data.frame(x = xvalues),interval='confidence', level=0.99)
```

Now we're ready to plot our model. We can start by plotting the data and true function as before.

```{r, tidy=TRUE, eval=FALSE}
plot(y = RcticG1ind1, x = xvalues, main = "Polynomial Regression", ylab = "Length (feet)", xlab = "Age (weeks)", las = 1)
curve(RcticG(Mono1akg[1,1], Mono1akg[1,2], Mono1akg[1,3], x), n = 101, add = TRUE, col = "red")
```

And then add the approximate function and a legend.

```{r, tidy=TRUE, eval=FALSE}
lines(fitted(SinModelPop1ind1)~xvalues, col='green', lwd=3)


lines(xvalues, PredictedIntervalsSin[,2],col=4, lwd=1, lty=2)
lines(xvalues, PredictedIntervalsSin[,3],col=4, lwd=1, lty=2)

legend("bottomright",c("Measured Length", "True length", "Predicted length", "99% Confidence Interval"), col = c("black", "red", "green", 4), pch = c(1, NA, NA, NA), lwd = c(NA, 1, 3, 1), lty = c(1, 1, 1, 2))

```

```{r, echo=FALSE, fig.cap=fig$cap("Growth of spinach leaves1", "Cosine decomposition")}
plot(y = RcticG1ind1, x = xvalues, main = NULL, ylab = "Length (feet)", xlab = "Age (weeks)", las = 1)
curve(RcticG(Mono1akg[1,1], Mono1akg[1,2], Mono1akg[1,3], x), n = 101, add = TRUE, col = "red")
lines(fitted(SinModelPop1ind1)~xvalues, col='green', lwd=3)
lines(xvalues, PredictedIntervalsSin[,2],col=4, lwd=1, lty=2)
lines(xvalues, PredictedIntervalsSin[,3],col=4, lwd=1, lty=2)
legend("bottomright",c("Measured Length", "True Length", "Predicted Length", "99% Confidence Interval"), col = c("black", "red", "green", 4), pch = c(1, NA, NA, NA), lwd = c(NA, 1, 3, 1), lty = c(1, 1, 1, 2))

```

This fit is comparable to a third degree polynomial which isn't so great. Let's see if we can improve the fit by adding a second harmonic.

```{r, include=FALSE}
SinModel2Pop1ind1 <- lm(RcticG1ind1 ~ sin(2*pi/period*xvalues)+cos(2*pi/period*xvalues)+sin(4*pi/period*xvalues)+cos(4*pi/period*xvalues))

PredictedIntervalsSin2 <- predict(SinModel2Pop1ind1, data.frame(x = xvalues),interval='confidence', level=0.99)
```

Now let's plot this and see how our approximation improves.

```{r}
plot(y = RcticG1ind1, x = xvalues, main = NULL, ylab = "Length (feet)", xlab = "Age (weeks)", las = 1)
curve(RcticG(Mono1akg[1,1], Mono1akg[1,2], Mono1akg[1,3], x), n = 101, add = TRUE, col = "red")
lines(fitted(SinModelPop1ind1)~xvalues, col="black", lwd=1)
lines(fitted(SinModel2Pop1ind1)~xvalues, col='green', lwd=3)
lines(xvalues, PredictedIntervalsSin2[,2],col=4, lwd=1, lty=2)
lines(xvalues, PredictedIntervalsSin2[,3],col=4, lwd=1, lty=2)
legend("bottomright",c("Measured Length", "True Length", "First Harmonic", "Second Harmonic", "99% Confidence Interval"), col = c("black", "red", "black", "green", 4), pch = c(1, NA, NA, NA, NA), lwd = c(NA, 1, 1, 3, 1), lty = c(1, 1, 1, 1, 2))
```

###Under the Hood

Now just as we did with the polynomial regression we can evaluate this model and take a look at the inner workings.

```{r, eval=FALSE}
SinModel2Pop1ind1
```

Even though we are no longer dealing with polynomials, this call still returns `Coefficients`. Just as before these are the amplitudes of various functions making up the model. Below is a plot of the functions that are being added together to form the model.

```{r, echo=FALSE, eval=FALSE}
plot(1, type="n", xlab="", ylab="", xlim = c(0, 10), ylim = c(-2.0, 1.5))
curve(-0.3638*sin(2 * pi/period * x), xlim = c(-1, 11), ylim = c(-2, 2), add = TRUE, col = 1)
curve(-0.5538*cos(2 * pi/period * x), xlim = c(-1, 11), ylim = c(-2, 2), add = TRUE, col = 2)
curve(0.0227*sin(4 * pi/period * x), xlim = c(-1, 11), ylim = c(-2, 2), add = TRUE, col = 3)
curve(-0.3423 *cos(4 * pi/period * x), xlim = c(-1, 11), ylim = c(-2, 2), add = TRUE, col = 4)
abline(h=0.8813,col= "orange")
```

## Activities
###Exercises:
1. Manipulate the following function, adding/removing terms, changing coefficients etc. and plot the results to gain an intuition for polynomials.
```{r}
polynomial <- function(x) {
  0 + x + 2*x^2 + 3*x^3 +4*x^4 +5*x^5 + 6*x^6
}
```

###Practice Problems:
1. a) Open the sample dataset CatTemp1.csv representing the growth rate of Freija Fritillary caterpillars as a function of temperature and write a program that fits a polynomial to the data. (Growth rate (mg day^–1^))
      b) Use your program to fit a polynomial to the data in CatTemp2.csv representing the growth rate of the Stella Orange Tip as a function of temperature and compare these populations using multivariate and function-valued methods.
      c) Plot the data of both populations, and their polynomial approximations on the same graph. Does it look like there ought to be a 
    d) TroubleShooting: Iport the data into Excel and use the plotting function to fit a polynomial of bestfit to the data. Do Excel's coefficients match those you found in R?
    
2. Determine whether or not there is a significant diference between the datasets:
(Still need to get it to export nicely but I want this to be the cyclic functions that I have defned, and I want erratic measurements to make it difficult to test a significant difference between the populations without using FVT analysis).