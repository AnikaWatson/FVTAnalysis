---
title: "Introduction to Function-Valued Analysis"
author: "Anika Watson"
date: '2018-01-16'
runtime: shiny
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r functions, include=FALSE}
# A function for captioning and referencing images
fig <- local({
    i <- 0
    ref <- list()
    list(
        cap=function(refName, text) {
            i <<- i + 1
            ref[[refName]] <<- i
            paste("Figure ", i, ": ", text, sep="")
        },
        ref=function(refName) {
            ref[[refName]]
        })
})
``` 

```{r, include=FALSE}
library("scales")
library(shiny)
```

## Introduction

Many biological traits are not scalar variables but continuous functions (Griswold, Gomulkiewicz & Heckman, 2008; Stinchcombe & Kirkpatrick, 2012; Hadjipantelis et al., 2013). Size, for instance, can be described as a function of age, fur thickness, a function of ambient temperature, and so on. Scientific articles in the sphere of function-valued analysis argue that treating such traits as continuous functions provides substantially greater statistical power and other clear advantages over multivariate approaches (Stinchcombe & Kirkpatrick, 2012; Griswold, Gomulkiewicz & Heckman, 2008). The technique works by fitting a function to the data and then running statistics on the coefficients of the approximating function, rather than the data itself.

In a 1998 article W.G. Hill predicted that function-valued methods may eventuallly replace traditional multivariate methods in evolutionary biology. This guide aims to help introduce biolgists to the world of function-valued analysis through examples and hands-on activities.

The code used to generate the data is available in `DataGeneration.R`.

This introduction to function-valuied trait analysis is designed for biology students with some experience in R and a basic understanding of statistics. We begin by briefly introducing the theory behind function-valued analysis, and then promptly dive headlong into examples. At the end of this text there are recommended activities so that readers can acquaint themselves more fully with function-valued traits. These activities call on R code and data that has been generated in conjunction with this text.

## Examples

### Fitting a Function to Data

If you've ever hit "add trendline" and "display equation on chart" in excel then you've already done this... sort of. Here we will explore how to fit a function to data in R.

First, we need data. I have created a sample dataset comprised of the lengths of the mythical species of fish, R-ctic ch-R. This dataset has both random individual variance, and random measurement error, and details about how the data was generated have been outlined in the *Data Generation* document. Here is a plot of the data:

```{r chR, include=FALSE}
RcticChR <- function(a, k, g, x) {
  a*exp(-exp(-k*(x-g)))-exp(-exp(-k*(-g)))
}
inp <- seq(from = 0.5, to = 10, by = 0.5)
a1 <- 1
k1 <- 1
g1 <- 3
RcticChR1 <- data.frame(matrix(NA, nrow=10, ncol=20))
Mono1akg <- data.frame(matrix(NA, nrow=100, ncol=3))
for (i in 1:10) {
  set.seed(1234*i)
  MonoError <- rnorm(n=20, mean=0, sd=0.04) #measurement error
  IndiDev <- rnorm(n=3, mean=0, sd=0.2) #the individual's deviation from the pop. av.
  for (l in 1:20){
    RcticChR1[i,l] <- RcticChR((a1+IndiDev[1]), (k1+IndiDev[2]), (g1+IndiDev[3]), (l/2)) + MonoError[l]
  }
  Mono1akg[i,1] <- a1+IndiDev[1]
  Mono1akg[i,2] <- k1+IndiDev[2]
  Mono1akg[i,3] <- g1+IndiDev[3]
}
colnames(RcticChR1) <- inp

```

```{r pressure, echo=FALSE, fig.cap=fig$cap("R-ctic Ch-R 1", "A plot of the ten male R-ctic Ch-R sampled.")}

#cols <- c("#000000", "#332288", "#88CCEE", "#44AA99", "#117733", "#999933", "#DDCC77", "#CC6677", "#882255", "#AA4499")

cols <- c("#000000", 2, 3, 4, 5, 6, 7, 8, "#882255", "darkolivegreen2")

matplot(y = t(RcticChR1), type = 'p', lty = 1, pch = 16, main = "Males", ylab = "Length (cm)", xlab = "Age (months)", las = 1, col = alpha(cols, 0.5))
```

Now that we have data we can fit various models to them. Let's begin by fitting a series of polynomials to the data in a process called polynmial regression. For simplicity, we will focus on one individual, from one population of R-ctic ch-R. Let's take the first male, `RcticChR1ind1`.

```{r, include=FALSE}
RcticChR1ind1 <- rep(NA, 20)
for (i in 1:20) {
  RcticChR1ind1[i] <- RcticChR1[1,i]
}
```

```{r, echo = FALSE, fig.cap=fig$cap("RcticChR1ind1", "A plot of the true and measured length of RcticChR1ind1.")}
xvalues <- seq(from = 0.5, to = 10, by = 0.5) #This object will come up often so take 
plot(y = RcticChR1ind1, x = xvalues, main = "RcticChR1ind1", ylab = "Length (cm)", xlab = "Age (months)", las = 1)
curve(RcticChR(Mono1akg[1,1], Mono1akg[1,2], Mono1akg[1,3], x), n = 101, add = TRUE, col = "red")
legend("bottomright",c("Data", "True Function"), 
       col = c("black", "red"), pch = c(1, NA), lwd = c(NA, 1), lty = c(1, 1))

```


In order to fit our polynomial model to `RcticChR1ind1` we will use the linear models function in R. Unlike its name suggests, the "linear model" function in R can do far more than merely slap on the closest linear approximation! We can get it to fit a polynomial of whatever order we desire by specifying the highest exponent in the model. 

```{r}
#First we need to define the x-values,
#In this case they represent the points in time when scientists measured the lengths of the fish.
xvalues <- seq(from = 0.5, to = 10, by = 0.5) #This object will come up often so take note!
#Now let's create an object for this model
PolyModelPop1ind1 <- lm(RcticChR1ind1 ~ poly(x = xvalues, n = 6))
```

From this we can calculate the predicted 99% confidence intervals.

```{r}
predicted.intervals <- predict(PolyModelPop1ind1, data.frame(x=inp), interval='confidence', level=0.99)
```

Now that we have a model we can plot it against the data to see how it holds up. 

First let's plot the data.

```{r, eval=FALSE}
plot.new(y = RcticChR1ind1, x = xvalues, main = "Polynomial Regression", ylab = "Length (cm)", xlab = "Age (months)", las = 1)
```

Then add the secret function of the true length of RcticChR1ind1 used to generate the data.

```{r, eval=FALSE}
curve(RcticChR(Mono1akg[1,1], Mono1akg[1,2], Mono1akg[1,3], x), n = 101, add = TRUE, col = "red")
```

And we're ready to plot the approximated polynomial function.

```{r, eval=FALSE}
lines(inp,predicted.intervals[,1],col='green', lwd=3)
lines(inp,predicted.intervals[,2],col=4, lwd=1, lty=2)
lines(inp,predicted.intervals[,3],col=4, lwd=1, lty=2)
```

Throw in a legend and we're good to go.

```{r, eval=FALSE}
legend("bottomright",c("Data", "True Fun.", "Predicted Fun.", "99% Conf. Int."), 
       col = c("black", "red", "green", 4), pch = c(1, NA, NA, NA), lwd = c(NA, 1, 3, 1), lty = c(1, 1, 1, 2))
```

```{r, echo=FALSE}
plot(y = RcticChR1ind1, x = xvalues, main = "Polynomial Regression", ylab = "Length (cm)", xlab = "Age (months)", las = 1)
curve(RcticChR(Mono1akg[1,1], Mono1akg[1,2], Mono1akg[1,3], x), n = 101, add = TRUE, col = "red")
lines(inp,predicted.intervals[,1],col='green', lwd=3)
lines(inp,predicted.intervals[,2],col=4, lwd=1, lty=2)
lines(inp,predicted.intervals[,3],col=4, lwd=1, lty=2)
legend("bottomright",c("Data", "True Fun.", "Predicted Fun.", "99% Conf. Int."), 
       col = c("black", "red", "green", 4), pch = c(1, NA, NA, NA), lwd = c(NA, 1, 3, 1), lty = c(1, 1, 1, 2))


```

This green line seems like a reasonable fit. But what if we hadn't chosen n=6? How did that arbitrary decision impact the model? Slide the toggle below to see for yourself.

```{r, echo=FALSE}
sliderInput("numb", label = "Polynomial Degree", min = 1, max = 19, value = 6, step = 1, animate = T)
```


```{r, echo=FALSE}

renderPlot({
  PolyModelPop1ind1 <- lm(RcticChR1ind1 ~ poly(x = inp, n = input$numb))
  predicted.intervals <- predict(PolyModelPop1ind1, data.frame(x=inp), interval='confidence', level=0.99)
  
  plot(y = RcticChR1ind1, x = xvalues, main = "Polynomial Regression", ylab = "Length (cm)", xlab = "Age (months)", las = 1)
  curve(RcticChR(Mono1akg[1,1], Mono1akg[1,2], Mono1akg[1,3], x), n = 101, add = TRUE, col = "red")
  lines(inp,predicted.intervals[,1],col='green', lwd=3)
  lines(inp,predicted.intervals[,2],col=4, lwd=1, lty=2)
  lines(inp,predicted.intervals[,3],col=4, lwd=1, lty=2)
  legend("bottomright",c("Data", "True Fun.", "Predicted Fun.", "99% Conf. Int."), col = c("black", "red", "green", 4), pch = c(1, NA, NA, NA), lwd = c(NA, 1, 3, 1), lty = c(1, 1, 1, 2))
})

```

It is important to note what happpens when n = 19. Here the approximate function has been overparametrized to the extreme and has lost all predictive value. At n = 19 the model also loses statistical power because when the leading term is x^19^, there are 20 coefficients to analyze, the same as the number of datapoints. The best choice for n would be between 4 and 6.

### Analysis

Ok so we've got a model, but what is R actually doing? Let's see what is in this mysterious object `PolyModelPop1ind1`.

```{r}
PolyModelPop1ind1
```

Evaluating `PolyModelPop1ind1` we see that it is composed of seven `Coefficients`.
Under the hood, R is adding together functions we told it to combine, namely, a costant, x, x^2^, x^3^, etc. up to x^6^. These `Coeffficients` correspond to the amplitudes of the functions making up the green curve shown in the figures above. In other words they tell us how much of each function is being added. Below is a visualization of the functions present when n = 6.

```{r, echo=FALSE, fig.cap=fig$cap("Coefficients", "A plot of the functions making up the polynomial approximation of RcticChR1ind1's growth function.")}
plot(1, type="n", xlab="", ylab="", xlim = c(0, 10), ylim = c(-50, 75))
curve(1.63443*x, xlim = c(-1, 11), ylim = c(-55, 55), add = TRUE, col = 1)
curve(-0.58209*x^2, to = 10, xlim = c(-1, 11), ylim = c(-55, 55), add = TRUE, col = 2)
curve(-0.07315*x^3, to = 10, xlim = c(-1, 11), ylim = c(-55, 55), add = TRUE, col = 3)
curve(0.26519*x^4, to = 10, xlim = c(-1, 11), ylim = c(-55, 55), add = TRUE, col = 4)
curve(-0.02839*x^5, to = 10, xlim = c(-1, 11), ylim = c(-55, 55), add = TRUE, col = 5)
curve(0.03007*x^6, to = 10, xlim = c(-1, 11), ylim = c(-55, 55), add = TRUE, col = 6)
abline(h=0.67789,col= "orange")
legend("topright", c("Constant", "x", as.expression(bquote( ~ x^2 ~ "")), as.expression(bquote( ~ x^3 ~ "")), as.expression(bquote( ~ x^4 ~ "")), as.expression(bquote( ~ x^5 ~ "")), as.expression(bquote( ~ x^6 ~ ""))), col = c("orange", 1, 2, 3, 4, 5, 6), lty = c(1, 1, 1, 1, 1))
```

Many of these functions get very big, very quickly, but when added together they make up a fair approximation of the length of `RcticChR1ind1`.

Now that we have gotten R to describe the model function it's come up with we are ready to run our analyses. These `Coeffficients` correspond to the amplitudes of the functions making up the green curve shown in the figures above.

We are finally ready to broaden our scope from our friend 'RcticChR1ind1' and consider this example from a population level. By repeating the process outlined above for all of the samples of the lengths of the males, we can collect a dataset of coefficients.

```{r}
#Take the data frame of the sample lengths
RcticChR1

#and make an empty data frame for the coefficients
PolyModel1 <- data.frame(matrix(data=NA,nrow=7,ncol=10))

#Now we can fill the empty data frame with the coefficients from the RcticChR1 sample
for (l in 1:10) {
  PolyModel1[l] <- coefficients(lm(RcticChR1[l,] ~ poly(x = xvalues, n = 6)))
}

#Let's take a look at what we've got
PolyModel1
```

Here we see that the data frame of coefficients is much more concise than the data frame of the data, but this does not mean that information is lost.

Now let's run an analysis between this sample and that of the female Rctic ChR.
```{r, include=FALSE}
a2 <- 1.4
k2 <- 1
g2 <- 2.6

#For reference:
#population 1:
#a1 <- 1
#k1 <- 1
#g1 <- 3

#Once again we need to build a data frame for our data
MonoData2 <- data.frame(matrix(NA, nrow=100, ncol=20))

#and one to keep track of what a, k, and g are for each individual in this population
Mono2akg <- data.frame(matrix(NA, nrow=100, ncol=3))


#Time to fill these frames
for (i in 1:10) {
  set.seed(1234*i)
  MonoError <- rnorm(n=20, mean=0, sd=0.04) #measurement error
  IndiDev <- rnorm(n=3, mean=0, sd=0.2) #individual deviation from population mean
  for (l in 1:20){
    MonoData2[i,l] <- MonoGrowth(a2+IndiDev[1], k2+IndiDev[2], g2+IndiDev[3], (l/2)) + MonoError[l]
  }
  Mono2akg[i,1] <- a2+IndiDev[1]
  Mono2akg[i,2] <- k2+IndiDev[2]
  Mono2akg[i,3] <- g2+IndiDev[3]
}

#and make an empty data frame for the coefficients
PolyModel2 <- data.frame(matrix(data=NA,nrow=7,ncol=10))

#Now we can fill the empty data frame with the coefficients from the RcticChR1 sample
for (l in 1:10) {
  PolyModel2[l] <- coefficients(lm(Mono1ind[l,] ~ poly(x = xvalues, n = 6)))
}
```

```{r}
HotellingsT2(PolyModel1, PolyModel2)
```


### Fitting a Sinusoidal Function to Data

Okay so we fit a polynomial to the data, but is that really the best fit? In excel the user can choose from a selection of fits so let's try another. Here we will go beyond the capabilities of Excel and fit a sinusoidal model to the data.

To begin, we can use the function `spectrum` to get the spectral density of `RcticChR1ind1`.

```{r}
spctrm <- spectrum(RcticChR1ind1)
```

This plot shows the prominence of various frequencie in the data, from which can find the period of the most prominent frequency.

```{r}
period <- 1/spctrm$freq[spctrm$spec==max(spctrm$spec)]
```

Now we're ready to make a model. Even though we are now even further from a linear model than we were with the polynomial approximation, we are going to use the linear model function once again.

```{r}
SinModelPop1ind1 <- lm(RcticChR1ind1 ~ sin(2*pi/period*xvalues)+cos(2*pi/period*xvalues))
```

Again, let's find the predicted intervals.

```{r}
PredictedIntervalsSin <- predict(SinModelPop1ind1, data.frame(x = xvalues),interval='confidence', level=0.99)
```

Now we're ready to plot our model. We can start by plotting the data and true function as before.

```{r, tidy=TRUE, eval=FALSE}
plot(y = RcticChR1ind1, x = xvalues, main = "Polynomial Regression", ylab = "Length (m)", xlab = "Age (months)", las = 1)
curve(RcticChR(Mono1akg[1,1], Mono1akg[1,2], Mono1akg[1,3], x), n = 101, add = TRUE, col = "red")
```

And then add the approximate function and a legend.

```{r, tidy=TRUE, eval=FALSE}
lines(fitted(SinModelPop1ind1)~xvalues, col='green', lwd=3)


lines(xvalues, PredictedIntervalsSin[,2],col=4, lwd=1, lty=2)
lines(xvalues, PredictedIntervalsSin[,3],col=4, lwd=1, lty=2)

legend("bottomright",c("Data", "True Fun.", "Predicted Fun.", "99% Conf. Int."), col = c("black", "red", "green", 4), pch = c(1, NA, NA, NA), lwd = c(NA, 1, 3, 1), lty = c(1, 1, 1, 2))

```

```{r, echo=FALSE}
plot(y = RcticChR1ind1, x = xvalues, main = "Sinusoidal Regression", ylab = "Length (m)", xlab = "Age (months)", las = 1)
curve(RcticChR(Mono1akg[1,1], Mono1akg[1,2], Mono1akg[1,3], x), n = 101, add = TRUE, col = "red")
lines(fitted(SinModelPop1ind1)~xvalues, col='green', lwd=3)
lines(xvalues, PredictedIntervalsSin[,2],col=4, lwd=1, lty=2)
lines(xvalues, PredictedIntervalsSin[,3],col=4, lwd=1, lty=2)
legend("bottomright",c("Data", "True Fun.", "Predicted Fun.", "99% Conf. Int."), col = c("black", "red", "green", 4), pch = c(1, NA, NA, NA), lwd = c(NA, 1, 3, 1), lty = c(1, 1, 1, 2))

```

This fit is comparable to a third degree polynomial which isn't so great. Let's see if we can improve the fit by adding a second harmonic.

```{r, include=FALSE}
SinModel2Pop1ind1 <- lm(RcticChR1ind1 ~ sin(2*pi/period*xvalues)+cos(2*pi/period*xvalues)+sin(4*pi/period*xvalues)+cos(4*pi/period*xvalues))

PredictedIntervalsSin2 <- predict(SinModel2Pop1ind1, data.frame(x = xvalues),interval='confidence', level=0.99)
```

Now let's plot this and see how our approximation improves.

```{r}
plot(y = RcticChR1ind1, x = xvalues, main = "Sinusoidal Regression", ylab = "Length (m)", xlab = "Age (months)", las = 1)
curve(RcticChR(Mono1akg[1,1], Mono1akg[1,2], Mono1akg[1,3], x), n = 101, add = TRUE, col = "red")
lines(fitted(SinModelPop1ind1)~xvalues, col="black", lwd=1)
lines(fitted(SinModel2Pop1ind1)~xvalues, col='green', lwd=3)
lines(xvalues, PredictedIntervalsSin2[,2],col=4, lwd=1, lty=2)
lines(xvalues, PredictedIntervalsSin2[,3],col=4, lwd=1, lty=2)
legend("bottomright",c("Data", "True Fun.", "First Harmonic", "Second Harmonic", "99% Conf. Int."), col = c("black", "red", "black", "green", 4), pch = c(1, NA, NA, NA, NA), lwd = c(NA, 1, 1, 3, 1), lty = c(1, 1, 1, 1, 2))
```

###Under the Hood

Now just as we did with the polynomial regression we can evaluate this model and take a look at the inner workings.

```{r}
SinModel2Pop1ind1
```

Even though we are no longer dealing with polynomials, this call still returns `Coefficients`. Just as before these are the amplitudes of various functions making up the model. Below is a plot of the functions that are being added together to form the model.

```{r, echo=FALSE}
plot(1, type="n", xlab="", ylab="", xlim = c(0, 10), ylim = c(-2.0, 1.5))
curve(-0.3638*sin(2 * pi/period * x), xlim = c(-1, 11), ylim = c(-2, 2), add = TRUE, col = 1)
curve(-0.5538*cos(2 * pi/period * x), xlim = c(-1, 11), ylim = c(-2, 2), add = TRUE, col = 2)
curve(0.0227*sin(4 * pi/period * x), xlim = c(-1, 11), ylim = c(-2, 2), add = TRUE, col = 3)
curve(-0.3423 *cos(4 * pi/period * x), xlim = c(-1, 11), ylim = c(-2, 2), add = TRUE, col = 4)
abline(h=0.8813,col= "orange")
```

## Activities
